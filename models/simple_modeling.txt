{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-18T06:00:12.550986Z","iopub.execute_input":"2022-06-18T06:00:12.551883Z","iopub.status.idle":"2022-06-18T06:00:12.567193Z","shell.execute_reply.started":"2022-06-18T06:00:12.551845Z","shell.execute_reply":"2022-06-18T06:00:12.566309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertForSequenceClassification \nfrom sklearn.model_selection import train_test_split \nimport tensorflow as tf \n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:03:17.365183Z","iopub.execute_input":"2022-06-18T06:03:17.365801Z","iopub.status.idle":"2022-06-18T06:03:24.24639Z","shell.execute_reply.started":"2022-06-18T06:03:17.365763Z","shell.execute_reply":"2022-06-18T06:03:24.24552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split x and y\ndf = pd.read_csv(\"/kaggle/input/sms-spam-collection-dataset/spam.csv\", encoding='latin-1')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:00:12.837607Z","iopub.execute_input":"2022-06-18T06:00:12.838446Z","iopub.status.idle":"2022-06-18T06:00:12.869353Z","shell.execute_reply.started":"2022-06-18T06:00:12.838415Z","shell.execute_reply":"2022-06-18T06:00:12.868502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:00:15.698449Z","iopub.execute_input":"2022-06-18T06:00:15.698789Z","iopub.status.idle":"2022-06-18T06:00:15.719559Z","shell.execute_reply.started":"2022-06-18T06:00:15.698761Z","shell.execute_reply":"2022-06-18T06:00:15.718864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string \nfrom nltk.corpus import stopwords \ndef text_process(mess):\n    \"\"\" Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords \n    3. Returns a list of the cleaned text \"\"\" \n    STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'] # Check characters to see if they are in punctuation \n    nopunc = [char for char in mess if char not in string.punctuation] # Join the characters again to form the string.\n    nopunc = ''.join(nopunc) # Now just remove any stopwords \n    nopunc = nopunc.lower()\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n\n\ndef split_x_y(df):\n    df['text'] = df['v2'].apply(text_process)\n    \n    label_dict = {'ham': 0, 'spam' :1}\n    df['labels'] = df['v1'].map(label_dict)\n    \n    train_texts = df['text'].values\n    train_labels = df['labels'].values\n    \n    return train_texts, train_labels\n\n\ndef data_preprocess(df):\n    \n    train_texts, train_labels = split_x_y(df)\n    train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.05)\n    \n    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True) \n    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)\n    \n    train_dataset = tf.data.Dataset.from_tensor_slices(( dict(train_encodings), train_labels ))\n    val_dataset = tf.data.Dataset.from_tensor_slices(( dict(val_encodings), val_labels ))\n    return train_dataset, val_dataset","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:07:34.444627Z","iopub.execute_input":"2022-06-18T06:07:34.445327Z","iopub.status.idle":"2022-06-18T06:07:34.461722Z","shell.execute_reply.started":"2022-06-18T06:07:34.445280Z","shell.execute_reply":"2022-06-18T06:07:34.460637Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:08:36.361180Z","iopub.execute_input":"2022-06-18T06:08:36.361749Z","iopub.status.idle":"2022-06-18T06:08:40.761351Z","shell.execute_reply.started":"2022-06-18T06:08:36.361713Z","shell.execute_reply":"2022-06-18T06:08:40.760209Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataset, val_dataset = data_preprocess(df)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:10:00.099569Z","iopub.execute_input":"2022-06-18T06:10:00.100635Z","iopub.status.idle":"2022-06-18T06:10:10.098887Z","shell.execute_reply.started":"2022-06-18T06:10:00.100589Z","shell.execute_reply":"2022-06-18T06:10:10.098041Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained(model_name)\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nmodel.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn \n\nmodel.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:10:20.914006Z","iopub.execute_input":"2022-06-18T06:10:20.914591Z","iopub.status.idle":"2022-06-18T06:12:52.572740Z","shell.execute_reply.started":"2022-06-18T06:10:20.914554Z","shell.execute_reply":"2022-06-18T06:12:52.571944Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model.save('./my_model')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:13:05.874005Z","iopub.execute_input":"2022-06-18T06:13:05.874852Z","iopub.status.idle":"2022-06-18T06:13:22.739138Z","shell.execute_reply.started":"2022-06-18T06:13:05.874812Z","shell.execute_reply":"2022-06-18T06:13:22.738012Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"os.listdir('./my_model/')","metadata":{"execution":{"iopub.status.busy":"2022-06-18T06:13:43.108581Z","iopub.execute_input":"2022-06-18T06:13:43.109223Z","iopub.status.idle":"2022-06-18T06:13:43.115329Z","shell.execute_reply.started":"2022-06-18T06:13:43.109188Z","shell.execute_reply":"2022-06-18T06:13:43.114516Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n# training_args = TFTrainingArguments( output_dir='./results', # output directory\n#                                     num_train_epochs=3, # total number of training epochs \n#                                     per_device_train_batch_size=16, # batch size per device during training \n#                                     per_device_eval_batch_size=64, # batch size for evaluation\n#                                     warmup_steps=500, # number of warmup steps for learning rate scheduler\n#                                     weight_decay=0.01, # strength of weight decay \n#                                     logging_dir='./logs', # directory for storing logs\n#                                     logging_steps=10, ) \n\n# with training_args.strategy.scope(): \n#     model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n\n# trainer = TFTrainer( model=model, # the instantiated ðŸ¤— Transformers model to be trained \n#                     args=training_args, # training arguments, defined above \n#                     train_dataset=train_dataset, # training dataset\n#                     eval_dataset=val_dataset # evaluation dataset \n#                    )\n\n# trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-18T05:52:15.516591Z","iopub.execute_input":"2022-06-18T05:52:15.517202Z","iopub.status.idle":"2022-06-18T05:52:34.931168Z","shell.execute_reply.started":"2022-06-18T05:52:15.517167Z","shell.execute_reply":"2022-06-18T05:52:34.928515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://huggingface.co/transformers/v3.2.0/custom_datasets.html","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}